{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "        \n",
    "class ACVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.label_num = 4\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(1+self.label_num, 8, (3,9), (1,1))\n",
    "        self.conv1_bn = nn.BatchNorm2d(8)\n",
    "        self.conv1_gated = nn.Conv2d(1+self.label_num, 8, (3,9), (1,1))\n",
    "        self.conv1_gated_bn = nn.BatchNorm2d(8)\n",
    "        self.conv1_sigmoid = nn.sigmoid()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(8+self.label_num, 16, (4,8), (2,2))\n",
    "        self.conv2_bn = nn.BatchNorm2d(16)\n",
    "        self.conv2_gated = nn.Conv2d(8+self.label_num, 16, (4,8), (2,2))\n",
    "        self.conv2_gated_bn = nn.BatchNorm2d(16)\n",
    "        self.conv2_sigmoid = nn.sigmoid()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(12+self.label_num, 16, (4,8), (2,2))\n",
    "        self.conv3_bn = nn.BatchNorm2d(16)\n",
    "        self.conv3_gated = nn.Conv2d(12+self.label_num, 16, (4,8), (2,2))\n",
    "        self.conv3_gated_bn = nn.BatchNorm2d(16)\n",
    "        self.conv3_sigmoid = nn.sigmoid()\n",
    "        \n",
    "        self.conv4_mu = nn.Conv2d(16+self.label_num, 10//2, (9,5), (9,1))\n",
    "        self.conv4_logvar = nn.Conv2d(16+self.label_num, 10//2, (9,5), (9,1))\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(5+self.label_num, 16, (9,5), (9,1))\n",
    "        self.upconv1_bn = nn.BatchNorm2d(16)\n",
    "        self.upconv1_gated = nn.ConvTranspose2d(5+self.label_num, 16, (9,5), (9,1))\n",
    "        self.upconv1_gated_bn = nn.BatchNorm2d(16)\n",
    "        self.upconv1_sigmoid = nn.sigmoid()\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(16+self.label_num, 16, (4,8), (2,2))\n",
    "        self.upconv2_bn = nn.BatchNorm2d(16)\n",
    "        self.upconv2_gated = nn.ConvTranspose2d(16+self.label_num, 16, (4,8), (2,2))\n",
    "        self.upconv2_gated_bn = nn.BatchNorm2d(16)\n",
    "        self.upconv2_sigmoid = nn.sigmoid()\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(16+self.label_num, 8, (4,8), (2,2))\n",
    "        self.upconv3_bn = nn.BatchNorm2d(8)\n",
    "        self.upconv3_gated = nn.ConvTranspose2d(16+self.label_num, 8, (4,8), (2,2))\n",
    "        self.upconv3_gated_bn = nn.BatchNorm2d(8)\n",
    "        self.upconv3_sigmoid = nn.sigmoid()\n",
    "        \n",
    "        self.upconv4 = nn.ConvTranspose2d(8+self.label_num, 2//2, (9,5), (1,1))\n",
    "        self.upconv4 = nn.ConvTranspose2d(8+self.label_num, 2//2, (9,5), (1,1))\n",
    "        \n",
    "        # Auxiliary Classifier\n",
    "        self.ac_conv1 = nn.Conv2d(1, 8, (4,4), (2,2))\n",
    "        self.ac_conv1_bn = nn.BatchNorm2d(8)\n",
    "        self.ac_conv1_gated = nn.Conv2d(1, 8, (4,4), (2,2))\n",
    "        self.ac_conv1_gated_bn = nn.BatchNorm2d(8)\n",
    "        self.ac_conv1_sigmoid = nn.sigmoid()\n",
    "        \n",
    "        self.ac_conv2 = nn.Conv2d(8, 16, (4,4), (2,2))\n",
    "        self.ac_conv2_bn = nn.BatchNorm2d(16)\n",
    "        self.ac_conv2_gated = nn.Conv2d(8, 16, (4,4), (2,2))\n",
    "        self.ac_conv2_gated_bn = nn.BatchNorm2d(16)\n",
    "        self.ac_conv2_sigmoid = nn.sigmoid()\n",
    "        \n",
    "        self.ac_conv3 = nn.Conv2d(16, 32, (4,4), (2,2))\n",
    "        self.ac_conv3_bn = nn.BatchNorm2d(32)\n",
    "        self.ac_conv3_gated = nn.Conv2d(16, 32, (4,4), (2,2))\n",
    "        self.ac_conv3_gated_bn = nn.BatchNorm2d(32)\n",
    "        self.ac_conv3_sigmoid = nn.sigmoid()\n",
    "        \n",
    "        self.ac_conv4 = nn.Conv2d(32, 16, (4,4), (2,2))\n",
    "        self.ac_conv4_bn = nn.BatchNorm2d(16)\n",
    "        self.ac_conv4_gated = nn.Conv2d(32, 16, (4,4), (2,2))\n",
    "        self.ac_conv4_gated_bn = nn.BatchNorm2d(16)\n",
    "        self.ac_conv4_sigmoid = nn.sigmoid()\n",
    "        \n",
    "        self.ac_conv5 = nn.Conv2d(16, self.label_num, (1,4), (1,2))\n",
    "        self.ac_fc5 = nn.Linear(self.label_num * 32, self.label_num)\n",
    "\n",
    "    def encode(self, x, label):\n",
    "       \n",
    "        h1_ = self.conv1_bn(self.conv1(self.concat_label(x, label)))\n",
    "        h1_gated = self.conv1_gated_bn(self.conv1_gated(self.concat_label(x, label)))\n",
    "        h1 = torch.mul(h1_, self.conv1_sigmoid(h1_gated)) \n",
    "        \n",
    "        h2_ = self.conv2_bn(self.conv2(self.concat_label(x, label)))\n",
    "        h2_gated = self.conv2_gated_bn(self.conv2_gated(self.concat_label(h1, label)))\n",
    "        h2 = torch.mul(h2_, self.conv2_sigmoid(h2_gated)) \n",
    "        \n",
    "        h3_ = self.conv3_bn(self.conv3(self.concat_label(x, label)))\n",
    "        h3_gated = self.conv3_gated_bn(self.conv3_gated(self.concat_label(h2, label)))\n",
    "        h3 = torch.mul(h3_, F.sigmoid(h3_gated)) \n",
    "        h3 = torch.mul(h3_, self.conv3_sigmoid(h3_gated)) \n",
    "        \n",
    "        h4_mu = self.conv4_mu(self.concat_label(h3, label))\n",
    "        h4_logvar = self.conv4_logvar(self.concat_label(h3, label))\n",
    "       \n",
    "        return h4_mu, h4_logvar \n",
    "\n",
    "    def decode(self, z, label):\n",
    "        \n",
    "        h5_ = self.upconv1_bn(self.upconv1(self.concat_label(z, label)))\n",
    "        h5_gated = self.upconv1_gated_bn(self.upconv1(self.concat_label(z, label)))\n",
    "        h5 = torch.mul(h5_, self.upconv1_sigmoid(h5_gated)) \n",
    "        \n",
    "        h6_ = self.upconv2_bn(self.upconv2(self.concat_label(h5, label)))\n",
    "        h6_gated = self.upconv2_gated_bn(self.upconv2(self.concat_label(h5, label)))\n",
    "        h6 = torch.mul(h6_, self.upconv2_sigmoid(h6_gated)) \n",
    "        \n",
    "        h7_ = self.upconv3_bn(self.upconv3(self.concat_label(h6, label)))\n",
    "        h7_gated = self.upconv3_gated_bn(self.upconv3(self.concat_label(h6, label)))\n",
    "        h7 = torch.mul(h7_, self.upconv3_sigmoid(h7_gated)) \n",
    "        \n",
    "        h8_mu = self.upconv4_mu(self.concat_label(h7, label))\n",
    "        h8_logvar = self.upconv4_logvar(self.concat_label(h7, label))\n",
    "        \n",
    "        return h8_mu, h8_logvar\n",
    "    \n",
    "    def classify(self, x, label):\n",
    "        \n",
    "        h9_ = self.ac_conv1_bn(self.ac_conv1(x))\n",
    "        h9_gated = self.ac_conv1_gated_bn(self.ac_conv1_gated(x))\n",
    "        h9 = torch.mul(h9_, self.ac_conv1_sigmoid(h9_gated))\n",
    "        \n",
    "        h10_ = self.ac_conv2_bn(self.ac_conv2(h9))\n",
    "        h10_gated = self.ac_conv2_gated_bn(self.ac_conv2_gated(h9))\n",
    "        h10 = torch.mul(h10_, self.ac_conv2_sigmoid(h10_gated))\n",
    "        \n",
    "        h11_ = self.ac_conv3_bn(self.ac_conv3(h10))\n",
    "        h11_gated = self.ac_conv3_gated_bn(self.ac_conv3_gated(h10))\n",
    "        h11 = torch.mul(h11_, self.ac_conv3_sigmoid(h11_gated))\n",
    "        \n",
    "        h12_ = self.ac_conv4_bn(self.ac_conv4(h11))\n",
    "        h12_gated = self.ac_conv4_gated_bn(self.ac_conv4_gated(h11))\n",
    "        h12 = torch.mul(h12_, self.ac_conv4_sigmoid(h12_gated))\n",
    "        \n",
    "        h13_ = F.softmax(self.ac_conv5(h12))\n",
    "        h13 = torch.prod(h13_, dim=-1, keepdim=True)\n",
    "        \n",
    "        return h13.view(-1, self.label_numbel)\n",
    "        \n",
    "    def concat_label(self, x, label):\n",
    "        shape = x.shape\n",
    "        shape[1] = self.label_num\n",
    "        label_layer = torch.zeros(shape)\n",
    "        for i in range(len(x)):\n",
    "            label_layer[i, label[i]] = torch.ones((shape[-1:]))\n",
    "        label_layer = label_layer.to(self.device)\n",
    "        return torch.cat((x, label_layer), dim=1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        mu_enc, logvar_enc = self.encode(x, label)\n",
    "        z_enc = self.reparameterize(mu_enc, logvar_enc)\n",
    "        mu_dec, logvar_dec = self.decode(z_enc, label)\n",
    "        z_dec = self.reparameterize(mu_dec, logvar_dec)\n",
    "        p_label = self.classify(z_dec, label)\n",
    "        return z_dec, mu_enc, logvar_enc, p_label\n",
    "                   \n",
    "    # Reconstruction + KL divergence losses summed over all elements and batch\n",
    "    def train_model(x, label):\n",
    "        \n",
    "        x = x.to(self.device)\n",
    "        label_ = label.to(self.device)\n",
    "\n",
    "        recon_x, mu, logvar, p_label = self.forward(x, label)\n",
    "        t_label = self.classify(x)\n",
    "\n",
    "        # 1\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        # 2\n",
    "        AC_1 = F.binary_cross_entropy(label_, p_label) \n",
    "\n",
    "        # 3\n",
    "        AC_2 = F.binary_cross_entropy(label_, t_label) \n",
    "\n",
    "        return BCE + KLD + AC_1 + AC_2\n",
    "\n",
    "    def pred(x, label, label_target):\n",
    "        \n",
    "        shape = x.shape\n",
    "        x = x.view(-1, shape[0], shape[1], shape[2])\n",
    "        x.to(self.device)\n",
    "        \n",
    "        mu_enc, logvar_enc = self.encode(x, label)\n",
    "        z_enc = self.reparameterize(mu_enc, logvar_enc)\n",
    "        mu_dec, logvar_dec = self.decode(z_enc, label_target)\n",
    "        z_dec = self.reparameterize(mu_dec, logvar_dec)\n",
    "        \n",
    "        return z_dec\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
