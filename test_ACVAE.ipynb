{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import pyworld\n",
    "import librosa\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocess import *\n",
    "from model import *\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./model/model_mc32_fr1024\"\n",
    "model_name = \"model_mc32_fr1024\"\n",
    "\n",
    "data_dir = \"./data/voice_data\"\n",
    "voice_dir = [\"F4\", \"F5\", \"F6\", \"M2\"]\n",
    "\n",
    "_from = voice_dir.index(\"M2\")\n",
    "_to = voice_dir.index(\"F4\")\n",
    "\n",
    "output_dir = \"./converted_voices/test\"\n",
    "figure_dir = \"./figure/model_mc32_fr1024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 16000\n",
    "num_mcep = 36\n",
    "frame_period = 5.0\n",
    "n_frames = 1024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save(model, model_dir, model_name):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    torch.save(model.state_dict(), os.path.join(model_dir, model_name))\n",
    "    \n",
    "def model_load(model_dir, model_name):\n",
    "    model = ACVAE()\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir, model_name), map_location='cpu'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, s_label, t_label):\n",
    "    \n",
    "    print(\"Test\")\n",
    "    print(\"Converted: \" + voice_dir[s_label] + \" -> \" + voice_dir[t_label])\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    voice_path_s = os.path.join(data_dir, voice_dir[s_label])\n",
    "    voice_path_t = os.path.join(data_dir, voice_dir[t_label])\n",
    "    \n",
    "    files = os.listdir(voice_path_s)\n",
    "    \n",
    "    for i, file in enumerate(files):\n",
    "        if(file.count(\"wav\") == 0):\n",
    "            continue   \n",
    "        if ((i+1) % 20 == 0):\n",
    "            print(str(((i+1)*100)//len(files)) + \" %\")\n",
    "\n",
    "        wav, _ = librosa.load(os.path.join(voice_path_s, file), sr = sampling_rate, mono = True)\n",
    "        wav = wav_padding(wav = wav, sr = sampling_rate, frame_period = frame_period, multiple = 4)\n",
    "        f0, timeaxis, sp, ap = world_decompose(wav = wav, fs = sampling_rate, frame_period = frame_period)\n",
    "        coded_sp = world_encode_spectral_envelop(sp = sp, fs = sampling_rate, dim = num_mcep)\n",
    "        coded_sp_transposed = coded_sp.T\n",
    "\n",
    "        mcep_normalization_params_s = np.load(os.path.join(voice_path_s, \"mcep_\"+voice_dir[s_label]+\".npz\"))\n",
    "        mcep_mean_s = mcep_normalization_params_s['mean']\n",
    "        mcep_std_s = mcep_normalization_params_s['std']    \n",
    "        mcep_normalization_params_t = np.load(os.path.join(voice_path_t, \"mcep_\"+voice_dir[t_label]+\".npz\"))\n",
    "        mcep_mean_t = mcep_normalization_params_t['mean']\n",
    "        mcep_std_t = mcep_normalization_params_t['std']\n",
    "\n",
    "        coded_sp_norm = (coded_sp_transposed - mcep_mean_s) / mcep_std_s\n",
    "\n",
    "        x = torch.Tensor(coded_sp_norm).view(1, 1, coded_sp_norm.shape[0], coded_sp_norm.shape[1])\n",
    "\n",
    "        label_s_tensor = torch.Tensor(np.array([s_label])).view(1, 1)\n",
    "        label_t_tensor = torch.Tensor(np.array([t_label])).view(1, 1)\n",
    "\n",
    "        mu_enc, logvar_enc = model.encode(x, label_s_tensor)\n",
    "        z_enc = model.reparameterize(mu_enc, logvar_enc)\n",
    "        mu_dec, logvar_dec = model.decode(z_enc, label_t_tensor)\n",
    "        z_dec = model.reparameterize(mu_dec, logvar_dec)\n",
    "        z_dec = z_dec.data.numpy().reshape((coded_sp_norm.shape[0], coded_sp_norm.shape[1]))\n",
    "\n",
    "        coded_sp_converted = z_dec * mcep_std_t + mcep_mean_t\n",
    "\n",
    "        logf0s_normalization_params_s = np.load(os.path.join(voice_path_s, \"log_f0_\"+voice_dir[s_label]+\".npz\"))\n",
    "        logf0s_mean_s = logf0s_normalization_params_s['mean']\n",
    "        logf0s_std_s = logf0s_normalization_params_s['std']\n",
    "        logf0s_normalization_params_t = np.load(os.path.join(voice_path_t, \"log_f0_\"+voice_dir[t_label]+\".npz\"))\n",
    "        logf0s_mean_t = logf0s_normalization_params_t['mean']\n",
    "        logf0s_std_t = logf0s_normalization_params_t['std']\n",
    "\n",
    "        f0_converted = pitch_conversion(f0 = f0, mean_log_src = logf0s_mean_s, std_log_src = logf0s_std_s, mean_log_target = logf0s_mean_t, std_log_target = logf0s_std_t)\n",
    "\n",
    "        coded_sp_converted = coded_sp_converted.T\n",
    "        coded_sp_converted = np.ascontiguousarray(coded_sp_converted)\n",
    "        decoded_sp_converted = world_decode_spectral_envelop(coded_sp = coded_sp_converted, fs = sampling_rate)\n",
    "        wav_transformed = world_speech_synthesis(f0 = f0_converted, decoded_sp = decoded_sp_converted, ap = ap, fs = sampling_rate, frame_period = frame_period)\n",
    "        librosa.output.write_wav(os.path.join(output_dir, os.path.basename(file)), wav_transformed, sampling_rate)\n",
    "\n",
    "    print(\"Finish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Test\n",
      "Converted: M2 -> F4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aoi/Workspace/Python/ACVAE-VC/preprocess.py:184: RuntimeWarning: divide by zero encountered in log\n",
      "  f0_converted = np.exp((np.log(f0) - mean_log_src) / std_log_src * std_log_target + mean_log_target)\n",
      "/Users/aoi/Workspace/Python/ACVAE-VC/preprocess.py:184: RuntimeWarning: invalid value encountered in log\n",
      "  f0_converted = np.exp((np.log(f0) - mean_log_src) / std_log_src * std_log_target + mean_log_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 %\n",
      "76 %\n",
      "115 %\n",
      "154 %\n",
      "194 %\n",
      "Finish.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = model_load(model_dir, model_name)\n",
    "\n",
    "test(model, _from, _to)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
