{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import pyworld\n",
    "import librosa\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocess import *\n",
    "from model import *\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./model/model_mc32_fr1024\"\n",
    "model_name = \"model_mc32_fr1024\"\n",
    "\n",
    "data_dir = \"./data/voice_data\"\n",
    "# voice_dir = [\"F4\", \"F5\", \"F6\", \"M2\"]\n",
    "voice_dir = [\"test01\", \"test02\"]\n",
    "\n",
    "_from = voice_dir.index(\"test01\")\n",
    "_to = voice_dir.index(\"test02\")\n",
    "\n",
    "# output_dir = \"./converted_voices/model_mc32_fr1024_training_progress\"\n",
    "output_dir = \"./converted_voices/test\"\n",
    "\n",
    "# figure_dir = \"./figure/model_mc32_fr1024\"\n",
    "figure_dir = \"./figure/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (False):\n",
    "    for v in voice_dir:\n",
    "        print(\"Preprocess: \" + v)\n",
    "        preprocess_voice(os.path.join(data_dir, v), v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10 #10000\n",
    "batch_size = 4\n",
    "learning_rate =1e-3\n",
    "learning_rate_ = 1e-4\n",
    "learning_rate__ = 1e-5\n",
    "\n",
    "sampling_rate = 16000\n",
    "num_mcep = 36\n",
    "frame_period = 5.0\n",
    "n_frames = 1024 \n",
    "\n",
    "losses = []\n",
    "loss_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save(model, model_dir, model_name):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    torch.save(model.state_dict(), os.path.join(model_dir, model_name))\n",
    "    \n",
    "def model_load(model_dir, model_name):\n",
    "    model = ACVAE()\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir, model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(losses, epoch):        \n",
    "    if not os.path.exists(figure_dir):\n",
    "            os.makedirs(figure_dir)\n",
    "    losses = np.array(losses)\n",
    "    losses = losses.reshape(-1, 4)\n",
    "    x = np.linspace(0, len(losses), len(losses))\n",
    "    losses_label = (\"L1\", \"KLD\", \"AC_1\", \"AC_2\")\n",
    "    plt.figure()\n",
    "    plt.plot(x, losses[:,0], label=losses_label[0])\n",
    "    plt.plot(x, losses[:,1], label=losses_label[1])\n",
    "    plt.plot(x, losses[:,2], label=losses_label[2])\n",
    "    plt.plot(x, losses[:,3], label=losses_label[3])\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc='upper right', borderaxespad=0)\n",
    "    plt.savefig(figure_dir + \"/\" + \"epoch_{:05}\".format(epoch) + \".png\")\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.plot(x, losses[:,2], label=losses_label[2])\n",
    "    plt.plot(x, losses[:,3], label=losses_label[3])\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc='upper right', borderaxespad=0)\n",
    "    plt.savefig(figure_dir + \"/\" + \"epoch_{:05}_detail\".format(epoch) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(batchsize = 1, s = -1, t = -1):\n",
    "    x = []\n",
    "    label = []\n",
    "    for i in range(batchsize):\n",
    "        if (s == -1):\n",
    "            label_num = np.random.randint(len(voice_dir))\n",
    "        else:\n",
    "            label_num = s\n",
    "        voice_path = os.path.join(data_dir, voice_dir[label_num])\n",
    "        files = os.listdir(voice_path)\n",
    "        \n",
    "        frames = 0\n",
    "        while frames < n_frames:\n",
    "            \n",
    "            file = \"\"\n",
    "            while file.count(\"wav\") == 0:\n",
    "                file = np.random.choice(files)\n",
    "            wav, _ = librosa.load(os.path.join(voice_path, file), sr = sampling_rate, mono = True)\n",
    "            wav = wav_padding(wav = wav, sr = sampling_rate, frame_period = frame_period, multiple = 4)\n",
    "            f0, timeaxis, sp, ap = world_decompose(wav = wav, fs = sampling_rate, frame_period = frame_period)\n",
    "            coded_sp = world_encode_spectral_envelop(sp = sp, fs = sampling_rate, dim = num_mcep)\n",
    "            coded_sp_transposed = coded_sp.T\n",
    "            frames = np.shape(coded_sp_transposed)[1]\n",
    "            \n",
    "        mcep_normalization_params = np.load(os.path.join(voice_path, \"mcep_\"+voice_dir[label_num]+\".npz\"))\n",
    "        mcep_mean = mcep_normalization_params['mean']\n",
    "        mcep_std = mcep_normalization_params['std']\n",
    "        coded_sp_norm = (coded_sp_transposed - mcep_mean) / mcep_std\n",
    "            \n",
    "        start_ = np.random.randint(frames - n_frames + 1)\n",
    "        end_ = start_ + n_frames\n",
    "            \n",
    "        x.append(coded_sp_norm[:,start_:end_])\n",
    "        label.append(label_num)\n",
    "\n",
    "    return torch.Tensor(x).view(batchsize, 1, num_mcep, n_frames), torch.Tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_file(model, s_label, t_label, epoch):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    voice_path_s = os.path.join(data_dir, voice_dir[s_label])\n",
    "    voice_path_t = os.path.join(data_dir, voice_dir[t_label])\n",
    "    \n",
    "    files = os.listdir(voice_path_s)\n",
    "    file = \"\"\n",
    "    while file.count(\"wav\") == 0:\n",
    "        file = np.random.choice(files)\n",
    "        \n",
    "    wav, _ = librosa.load(os.path.join(voice_path_s, file), sr = sampling_rate, mono = True)\n",
    "    wav = wav_padding(wav = wav, sr = sampling_rate, frame_period = frame_period, multiple = 4)\n",
    "    f0, timeaxis, sp, ap = world_decompose(wav = wav, fs = sampling_rate, frame_period = frame_period)\n",
    "    coded_sp = world_encode_spectral_envelop(sp = sp, fs = sampling_rate, dim = num_mcep)\n",
    "    coded_sp_transposed = coded_sp.T\n",
    "    \n",
    "    mcep_normalization_params_s = np.load(os.path.join(voice_path_s, \"mcep_\"+voice_dir[s_label]+\".npz\"))\n",
    "    mcep_mean_s = mcep_normalization_params_s['mean']\n",
    "    mcep_std_s = mcep_normalization_params_s['std']    \n",
    "    mcep_normalization_params_t = np.load(os.path.join(voice_path_t, \"mcep_\"+voice_dir[t_label]+\".npz\"))\n",
    "    mcep_mean_t = mcep_normalization_params_t['mean']\n",
    "    mcep_std_t = mcep_normalization_params_t['std']\n",
    "    \n",
    "    coded_sp_norm = (coded_sp_transposed - mcep_mean_s) / mcep_std_s\n",
    "    \n",
    "    x = torch.Tensor(coded_sp_norm).view(1, 1, coded_sp_norm.shape[0], coded_sp_norm.shape[1])\n",
    "    \n",
    "    label_s_tensor = torch.Tensor(np.array([s_label])).view(1, 1)\n",
    "    label_t_tensor = torch.Tensor(np.array([t_label])).view(1, 1)\n",
    "    \n",
    "    x = x.to(device)\n",
    "    label_s_tensor = label_s_tensor.to(device)\n",
    "    label_t_tensor = label_t_tensor.to(device)\n",
    "    \n",
    "    mu_enc, logvar_enc = model.encode(x, label_s_tensor)\n",
    "    z_enc = model.reparameterize(mu_enc, logvar_enc)\n",
    "    mu_dec, logvar_dec = model.decode(z_enc, label_t_tensor)\n",
    "    z_dec = model.reparameterize(mu_dec, logvar_dec)\n",
    "    if (torch.cuda.is_available()):\n",
    "        z_dec = z_dec.data.cpu().numpy().reshape((coded_sp_norm.shape[0], coded_sp_norm.shape[1]))\n",
    "    else:\n",
    "        z_dec = z_dec.data.numpy().reshape((coded_sp_norm.shape[0], coded_sp_norm.shape[1]))\n",
    "    \n",
    "    coded_sp_converted = z_dec * mcep_std_t + mcep_mean_t\n",
    "    \n",
    "    logf0s_normalization_params_s = np.load(os.path.join(voice_path_s, \"log_f0_\"+voice_dir[s_label]+\".npz\"))\n",
    "    logf0s_mean_s = logf0s_normalization_params_s['mean']\n",
    "    logf0s_std_s = logf0s_normalization_params_s['std']\n",
    "    logf0s_normalization_params_t = np.load(os.path.join(voice_path_t, \"log_f0_\"+voice_dir[t_label]+\".npz\"))\n",
    "    logf0s_mean_t = logf0s_normalization_params_t['mean']\n",
    "    logf0s_std_t = logf0s_normalization_params_t['std']\n",
    "    \n",
    "    f0_converted = pitch_conversion(f0 = f0, mean_log_src = logf0s_mean_s, std_log_src = logf0s_std_s, mean_log_target = logf0s_mean_t, std_log_target = logf0s_std_t)\n",
    "    \n",
    "    coded_sp_converted = coded_sp_converted.T\n",
    "    coded_sp_converted = np.ascontiguousarray(coded_sp_converted)\n",
    "    decoded_sp_converted = world_decode_spectral_envelop(coded_sp = coded_sp_converted, fs = sampling_rate)\n",
    "    wav_transformed = world_speech_synthesis(f0 = f0_converted, decoded_sp = decoded_sp_converted, ap = ap, fs = sampling_rate, frame_period = frame_period)\n",
    "    librosa.output.write_wav(os.path.join(output_dir, \"epoch_{:05}\".format(epoch) + \"_\" + os.path.basename(file)), wav_transformed, sampling_rate)\n",
    "    \n",
    "    print(\"Test\")\n",
    "    print(\"Source File:\" + file)\n",
    "    print(\"Converted: \" + voice_dir[s_label] + \" -> \" + voice_dir[t_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got -0.089373 at /Users/soumith/miniconda2/conda-bld/pytorch_1532623076075/work/aten/src/THNN/generic/BCECriterion.c:60",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f175b9bee337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/Python/ACVAE-VC/model.py\u001b[0m in \u001b[0;36mcalc_loss\u001b[0;34m(self, x, label)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mBCE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruction_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;31m# L1 = torch.mean(torch.abs(recon_x - x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# l1_loss = nn.SmoothL1Loss().to(self.device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/Python/ACVAE-VC/model.py\u001b[0m in \u001b[0;36mreconstruction_loss\u001b[0;34m(self, x_reconstructed, x)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreconstruction_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_reconstructed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_reconstructed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# Reconstruction + KL divergence losses summed over all elements and batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got -0.089373 at /Users/soumith/miniconda2/conda-bld/pytorch_1532623076075/work/aten/src/THNN/generic/BCECriterion.c:60"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = ACVAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch += 1\n",
    "     \n",
    "    if (epoch == 2000):\n",
    "        learning_rate = learning_rate_   \n",
    "    if (epoch == 5000):\n",
    "        learning_rate = learning_rate__\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    print('Epoch: %d' % epoch)\n",
    "\n",
    "    x_, label_ = data_load(batch_size)\n",
    "    optimizer.zero_grad()\n",
    "    loss, loss_list = model.calc_loss(x_, label_)\n",
    "    loss.backward()\n",
    "    losses.append(loss_list)\n",
    "    optimizer.step()\n",
    "    loss_num += 1\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        test_one_file(model, _from, _to, epoch)\n",
    "    if epoch % 100 == 0:\n",
    "        model_save(model, model_dir, model_name)\n",
    "    if epoch % 2000 == 0:\n",
    "        model_save(model, model_dir, model_name + \"_\" + str(epoch))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        save_figure(losses, epoch)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Time Elapsed for one epoch: %02d:%02d:%02d' % (elapsed_time // 3600, (elapsed_time % 3600 // 60), (elapsed_time % 60 // 1)))\n",
    "\n",
    "model_save(model, model_dir, model_name)\n",
    "\n",
    "save_figure(losses, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b= data_load(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(a[0,0].flatten()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
